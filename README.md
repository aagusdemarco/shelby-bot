# SHELBY BOT 
#### Video Demo:
#### Description:

**Shelby Bot** is an automated Twitter bot that posts quotes from the TV show *Peaky Blinders* twice a day. Built using a combination of Node.js and Python, it leverages the Twitter API v2 for tweeting, SQLite3 for data storage, and GitHub Actions for automation. The main functionality of this bot is to scrape quotes from a specific website, store them in a database, and then post them on Twitter at scheduled intervals. The tweets are scheduled to be posted at 8:00 AM and 4:00 PM GMT-3 every day, ensuring a consistent stream of content for followers.

The primary script that powers the bot is `index.js`, a Node.js application responsible for fetching random quotes from the database and posting them on Twitter. The script utilizes several libraries such as `dotenv` to manage environment variables securely, `sqlite3` to interact with the SQLite database, and a custom `twitterClient.js` module to handle the Twitter API integration. The `index.js` script contains three main functions: `getQuote()`, which retrieves a random quote from the database using SQL commands; `tweet()`, which posts the retrieved quote to Twitter via the Twitter API; and `postQuote()`, which orchestrates the overall process by combining the previous two functions. Together, these components work seamlessly to automate the process of posting quotes.

The quotes are sourced and stored using a Python script named `scraper.py`. This script employs BeautifulSoup (bs4) and Requests, two popular Python libraries for web scraping. The script fetches quotes from a webpage that lists memorable quotes from *Peaky Blinders* and parses the HTML content to extract the quotes. The cleaned and formatted quotes are then stored in an SQLite3 database (`shelby-quotes.db`). This database acts as a lightweight, serverless storage solution that the bot can easily query to retrieve random quotes. By utilizing SQLite3, the project ensures ease of setup and portability, making it simple for others to replicate or adapt the bot.

The `twitterClient.js` file is another crucial component of the project. It handles the connection and authentication to the Twitter API v2, providing methods necessary for interacting with Twitter programmatically. This file abstracts away the complexity of dealing with the Twitter API directly, making the `index.js` script more streamlined and focused on its core functionality.

For managing dependencies and project configurations, the project includes `package.json` and `package-lock.json` files. These files specify the required libraries and their versions, ensuring that the environment is correctly set up for the bot to run smoothly. Additionally, they provide scripts to handle various tasks, such as starting the bot or installing dependencies.

A key feature of Shelby Bot is its automation, which is achieved using GitHub Actions. The workflow is defined in the `.github/workflows/bot.yml` file. This file contains the instructions for GitHub Actions to execute the bot at specified times—8:00 AM and 4:00 PM GMT-3—every day. The workflow involves several steps, including checking out the repository, setting up Node.js, installing dependencies, configuring environment variables from GitHub Secrets, and finally running the `index.js` script. By leveraging GitHub Actions, the project avoids the need for an always-on server or manual intervention, making it an efficient and automated solution.

The project also relies on a `.env` file (not included in the repository for security reasons) that contains sensitive information such as API keys and tokens for authenticating with the Twitter API. Users who wish to run the bot locally or on their servers must create this file based on a provided template, ensuring the security of their credentials.

In terms of design, this project utilizes both Python and JavaScript to play to the strengths of each language. Python, with its robust libraries for web scraping, handles the data collection, while JavaScript (Node.js) is well-suited for handling asynchronous API calls, such as those required to interact with Twitter. The decision to use SQLite3 as the database was driven by its simplicity and low overhead, which fits the needs of this relatively lightweight project.

To syntesize, Shelby Bot is a robust yet straightforward automation tool that combines web scraping, database management, and social media automation into a single, cohesive project. It effectively demonstrates the power of combining different programming languages and tools to create an automated solution for social media content management. Whether for learning or practical application, this project provides a solid foundation for those interested in web scraping, automation, or bot development.
